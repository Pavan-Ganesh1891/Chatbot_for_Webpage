{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(query=\"2405.17147\")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 100)\n",
    "chunk_data = text_splitter.split_documents(docs)\n",
    "len(chunk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_cohere in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (0.2.4)\n",
      "Requirement already satisfied: cohere<6.0,>=5.5.6 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain_cohere) (5.9.1)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2.33 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain_cohere) (0.2.39)\n",
      "Requirement already satisfied: langchain-experimental>=0.0.6 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain_cohere) (0.0.65)\n",
      "Requirement already satisfied: pandas>=1.4.3 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain_cohere) (2.2.2)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain_cohere) (0.9.0)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (1.35.18)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (1.9.7)\n",
      "Requirement already satisfied: httpx>=0.21.2 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (0.27.2)\n",
      "Requirement already satisfied: httpx-sse==0.4.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (0.4.0)\n",
      "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (0.9.0)\n",
      "Requirement already satisfied: pydantic>=1.9.2 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (2.9.1)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (2.23.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1,>=0.15 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (0.20.0)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (2.32.0.20240907)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (4.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.33->langchain_cohere) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.33->langchain_cohere) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.33->langchain_cohere) (0.1.120)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.33->langchain_cohere) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain-core<0.3,>=0.2.33->langchain_cohere) (8.5.0)\n",
      "Requirement already satisfied: langchain-community<0.3.0,>=0.2.16 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain-experimental>=0.0.6->langchain_cohere) (0.2.16)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from pandas>=1.4.3->langchain_cohere) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from pandas>=1.4.3->langchain_cohere) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from pandas>=1.4.3->langchain_cohere) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from pandas>=1.4.3->langchain_cohere) (2024.1)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.18 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere) (1.35.18)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere) (0.10.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain_cohere) (4.4.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain_cohere) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain_cohere) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain_cohere) (3.8)\n",
      "Requirement already satisfied: sniffio in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain_cohere) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.21.2->cohere<6.0,>=5.5.6->langchain_cohere) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.33->langchain_cohere) (3.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (3.10.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.16 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (0.2.16)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3,>=0.2.33->langchain_cohere) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from pydantic>=1.9.2->cohere<6.0,>=5.5.6->langchain_cohere) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.4.3->langchain_cohere) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.5.6->langchain_cohere) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.0.0->cohere<6.0,>=5.5.6->langchain_cohere) (2.2.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain_cohere) (0.24.7)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (1.11.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (0.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain_cohere) (3.16.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain_cohere) (2024.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain_cohere) (4.66.5)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from langchain<0.3.0,>=0.2.16->langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (0.2.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (3.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere<6.0,>=5.5.6->langchain_cohere) (0.4.6)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\bmpav\\downloads\\rag-demo\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.2.16->langchain-experimental>=0.0.6->langchain_cohere) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "!pip install langchain_cohere\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "# Initialize Cohere embeddings\n",
    "cohere_embeddings = CohereEmbeddings(model=\"embed-english-v2.0\", cohere_api_key=\"pGdW2vRcRYnh8PKvJoZ5Slm8j0p1ZNHHw8bYVQTo\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "index_name = \"rag2\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name = index_name,\n",
    "        dimension=4096,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud = \"aws\", region=\"us-east-1\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embed and store\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "vector_store = PineconeVectorStore.from_documents(\n",
    "        chunk_data, \n",
    "        cohere_embeddings, \n",
    "        index_name=index_name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}, page_content='CTSOC NEWS ON CONSUMER TECHNOLOGY\\n1\\nLarge Language Models (LLMs):\\nDeployment, Tokenomics and Sustainability\\nHaiwei Dong Senior Member, IEEE, Shuang Xie Member, IEEE\\nAbstract—The rapid advancement of Large Language Models\\n(LLMs) has significantly impacted human-computer interaction,\\nepitomized by the release of GPT-4o, which introduced com-\\nprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations,\\nand sustainability challenges associated with the state-of-the-art\\nLLMs. More specifically, we discussed the deployment debate\\nbetween Retrieval-Augmented Generation (RAG) and fine-tuning,\\nhighlighting their respective advantages and limitations. After\\nthat, we quantitatively analyzed the requirement of xPUs in\\ntraining and inference. Additionally, for the tokenomics of LLM\\nservices, we examined the balance between performance and cost\\nfrom the quality of experience (QoE)’s perspective of end users.'),\n",
       " Document(metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}, page_content='from the quality of experience (QoE)’s perspective of end users.\\nLastly, we envisioned the future hybrid architecture of LLM\\nprocessing and its corresponding sustainability concerns, partic-\\nularly in the environmental carbon footprint impact. Through\\nthese discussions, we provided a comprehensive overview of\\nthe operational and strategic considerations essential for the\\nresponsible development and deployment of LLMs.\\nI. UBIQUITOUS LLMS\\nThe recent unveiling of GPT-4o by OpenAI on May 13,\\n2024 marks a pivotal moment in the evolution of large\\nlanguage models (LLMs) [1]. This groundbreaking model,\\naptly named with “o” signifying “omni” for its comprehensive\\ncapabilities, transcends the limitations of its predecessors by\\nincorporating multi modality. This signifies a significant step\\ntowards achieving more natural and intuitive human-computer\\ninteraction.\\nThe emergence of LLMs started from the launch of Chat-\\nGPT in November 2022 after two months of which, it reached'),\n",
       " Document(metadata={'Authors': 'Haiwei Dong, Shuang Xie', 'Published': '2024-05-27', 'Summary': \"The rapid advancement of Large Language Models (LLMs) has significantly\\nimpacted human-computer interaction, epitomized by the release of GPT-4o, which\\nintroduced comprehensive multi-modality capabilities. In this paper, we first\\nexplored the deployment strategies, economic considerations, and sustainability\\nchallenges associated with the state-of-the-art LLMs. More specifically, we\\ndiscussed the deployment debate between Retrieval-Augmented Generation (RAG)\\nand fine-tuning, highlighting their respective advantages and limitations.\\nAfter that, we quantitatively analyzed the requirement of xPUs in training and\\ninference. Additionally, for the tokenomics of LLM services, we examined the\\nbalance between performance and cost from the quality of experience (QoE)'s\\nperspective of end users. Lastly, we envisioned the future hybrid architecture\\nof LLM processing and its corresponding sustainability concerns, particularly\\nin the environmental carbon footprint impact. Through these discussions, we\\nprovided a comprehensive overview of the operational and strategic\\nconsiderations essential for the responsible development and deployment of\\nLLMs.\", 'Title': 'Large Language Models (LLMs): Deployment, Tokenomics and Sustainability'}, page_content='100 milliseconds per token, equivalent to 10 tokens per\\nsecond, leading to 600 words per minute that is faster\\nthan most of the users can read.\\n• Latency:\\nThe\\noverall\\ntime\\nof\\nreceiving\\nan\\nan-\\nswer\\ncomposed\\nof\\ntokens\\nafter\\nthe\\ninitial\\nquery\\nfrom\\nthe\\nusers.\\nTo\\nbe\\nmore\\nprecise,\\nit\\ncan\\nbe\\ndescribed\\nas\\nLatency\\n=\\nTTFT\\n+ TOPT\\n∗\\n(number of total generated tokens).\\n• Throughput: The generated tokens per second. It can\\nbe considered as a median value in a time window\\nconsidering applicable queries and users.\\nIt is important for users to receive high QoE while using\\nthe LLM service. However, better service requires more com-\\nputational and networking resources. It is speculated that the\\nLLM service will be provided with different levels relating\\nto various subscription options or an one-time charge from\\nusers. It would be finally a compromise decision of users in\\nthe consideration of performance and cost.\\nV. HYBRID LLM: GRAVITATING TOWARDS THE EDGE')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What metrics are used to evaluate the quality of experience (QoE) for users of large language model (LLM) service\"\n",
    "\n",
    "retriever = vector_store.as_retriever(search_kwargs = {\"k\":3})\n",
    "retriever.get_relevant_documents(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTSOC NEWS ON CONSUMER TECHNOLOGY\n",
      "1\n",
      "Large Language Models (LLMs):\n",
      "Deployment, Tokenomics and Sustainability\n",
      "Haiwei Dong Senior Member, IEEE, Shuang Xie Member, IEEE\n",
      "Abstract—The rapid advancement of Large Language Models\n",
      "(LLMs) has significantly impacted human-computer interaction,\n",
      "epitomized by the release of GPT-4o, which introduced com-\n",
      "prehensive multi-modality capabilities. In this paper, we first\n",
      "explored the deployment strategies, economic considerations,\n",
      "and sustainability challenges associated with the state-of-the-art\n",
      "LLMs. More specifically, we discussed the deployment debate\n",
      "between Retrieval-Augmented Generation (RAG) and fine-tuning,\n",
      "highlighting their respective advantages and limitations. After\n",
      "that, we quantitatively analyzed the requirement of xPUs in\n",
      "training and inference. Additionally, for the tokenomics of LLM\n",
      "services, we examined the balance between performance and cost\n",
      "from the quality of experience (QoE)’s perspective of end users.\n",
      "\n",
      "CTSOC NEWS ON CONSUMER TECHNOLOGY\n",
      "1\n",
      "Large Language Models (LLMs):\n",
      "Deployment, Tokenomics and Sustainability\n",
      "Haiwei Dong Senior Member, IEEE, Shuang Xie Member, IEEE\n",
      "Abstract—The rapid advancement of Large Language Models\n",
      "(LLMs) has significantly impacted human-computer interaction,\n",
      "epitomized by the release of GPT-4o, which introduced com-\n",
      "prehensive multi-modality capabilities. In this paper, we first\n",
      "explored the deployment strategies, economic considerations,\n",
      "and sustainability challenges associated with the state-of-the-art\n",
      "LLMs. More specifically, we discussed the deployment debate\n",
      "between Retrieval-Augmented Generation (RAG) and fine-tuning,\n",
      "highlighting their respective advantages and limitations. After\n",
      "that, we quantitatively analyzed the requirement of xPUs in\n",
      "training and inference. Additionally, for the tokenomics of LLM\n",
      "services, we examined the balance between performance and cost\n",
      "from the quality of experience (QoE)’s perspective of end users.\n",
      "\n",
      "from the quality of experience (QoE)’s perspective of end users.\n",
      "Lastly, we envisioned the future hybrid architecture of LLM\n",
      "processing and its corresponding sustainability concerns, partic-\n",
      "ularly in the environmental carbon footprint impact. Through\n",
      "these discussions, we provided a comprehensive overview of\n",
      "the operational and strategic considerations essential for the\n",
      "responsible development and deployment of LLMs.\n",
      "I. UBIQUITOUS LLMS\n",
      "The recent unveiling of GPT-4o by OpenAI on May 13,\n",
      "2024 marks a pivotal moment in the evolution of large\n",
      "language models (LLMs) [1]. This groundbreaking model,\n",
      "aptly named with “o” signifying “omni” for its comprehensive\n",
      "capabilities, transcends the limitations of its predecessors by\n",
      "incorporating multi modality. This signifies a significant step\n",
      "towards achieving more natural and intuitive human-computer\n",
      "interaction.\n",
      "The emergence of LLMs started from the launch of Chat-\n",
      "GPT in November 2022 after two months of which, it reached\n"
     ]
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "print(format_docs(retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are an expert LLM assistant specialized in answering questions related to large language models (LLMs). Use the provided information and your knowledge to respond accurately and clearly to each question. \n",
    "\n",
    "Guidelines:\n",
    "1. Provide concise and informative answers.\n",
    "2. If the question is beyond the scope of your knowledge or the provided information, state, \"I don't know.\"\n",
    "3. Use examples where applicable to illustrate your answers.\n",
    "4. Maintain a professional and helpful tone.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context:{context}\n",
    "\n",
    "Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert LLM assistant specialized in answering questions related to large language models (LLMs). Use the provided information and your knowledge to respond accurately and clearly to each question. \n",
      "\n",
      "Guidelines:\n",
      "1. Provide concise and informative answers.\n",
      "2. If the question is beyond the scope of your knowledge or the provided information, state, \"I don't know.\"\n",
      "3. Use examples where applicable to illustrate your answers.\n",
      "4. Maintain a professional and helpful tone.\n",
      "\n",
      "Question: What metrics are used to evaluate the quality of experience (QoE) for users of large language model (LLM) service\n",
      "\n",
      "Context:CTSOC NEWS ON CONSUMER TECHNOLOGY\n",
      "1\n",
      "Large Language Models (LLMs):\n",
      "Deployment, Tokenomics and Sustainability\n",
      "Haiwei Dong Senior Member, IEEE, Shuang Xie Member, IEEE\n",
      "Abstract—The rapid advancement of Large Language Models\n",
      "(LLMs) has significantly impacted human-computer interaction,\n",
      "epitomized by the release of GPT-4o, which introduced com-\n",
      "prehensive multi-modality capabilities. In this paper, we first\n",
      "explored the deployment strategies, economic considerations,\n",
      "and sustainability challenges associated with the state-of-the-art\n",
      "LLMs. More specifically, we discussed the deployment debate\n",
      "between Retrieval-Augmented Generation (RAG) and fine-tuning,\n",
      "highlighting their respective advantages and limitations. After\n",
      "that, we quantitatively analyzed the requirement of xPUs in\n",
      "training and inference. Additionally, for the tokenomics of LLM\n",
      "services, we examined the balance between performance and cost\n",
      "from the quality of experience (QoE)’s perspective of end users.\n",
      "\n",
      "CTSOC NEWS ON CONSUMER TECHNOLOGY\n",
      "1\n",
      "Large Language Models (LLMs):\n",
      "Deployment, Tokenomics and Sustainability\n",
      "Haiwei Dong Senior Member, IEEE, Shuang Xie Member, IEEE\n",
      "Abstract—The rapid advancement of Large Language Models\n",
      "(LLMs) has significantly impacted human-computer interaction,\n",
      "epitomized by the release of GPT-4o, which introduced com-\n",
      "prehensive multi-modality capabilities. In this paper, we first\n",
      "explored the deployment strategies, economic considerations,\n",
      "and sustainability challenges associated with the state-of-the-art\n",
      "LLMs. More specifically, we discussed the deployment debate\n",
      "between Retrieval-Augmented Generation (RAG) and fine-tuning,\n",
      "highlighting their respective advantages and limitations. After\n",
      "that, we quantitatively analyzed the requirement of xPUs in\n",
      "training and inference. Additionally, for the tokenomics of LLM\n",
      "services, we examined the balance between performance and cost\n",
      "from the quality of experience (QoE)’s perspective of end users.\n",
      "\n",
      "from the quality of experience (QoE)’s perspective of end users.\n",
      "Lastly, we envisioned the future hybrid architecture of LLM\n",
      "processing and its corresponding sustainability concerns, partic-\n",
      "ularly in the environmental carbon footprint impact. Through\n",
      "these discussions, we provided a comprehensive overview of\n",
      "the operational and strategic considerations essential for the\n",
      "responsible development and deployment of LLMs.\n",
      "I. UBIQUITOUS LLMS\n",
      "The recent unveiling of GPT-4o by OpenAI on May 13,\n",
      "2024 marks a pivotal moment in the evolution of large\n",
      "language models (LLMs) [1]. This groundbreaking model,\n",
      "aptly named with “o” signifying “omni” for its comprehensive\n",
      "capabilities, transcends the limitations of its predecessors by\n",
      "incorporating multi modality. This signifies a significant step\n",
      "towards achieving more natural and intuitive human-computer\n",
      "interaction.\n",
      "The emergence of LLMs started from the launch of Chat-\n",
      "GPT in November 2022 after two months of which, it reached\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(question = query, context =  format_docs(retrieved_docs))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': \"Evaluating Quality of Experience (QoE) in Large Language Model (LLM) services is a complex task as it depends on various factors that influence user satisfaction. Here's an overview of some commonly considered aspects:\\n\\n1. **Response Accuracy**: The most fundamental metric, response accuracy measures how well the LLM provides correct and relevant information based on the input query or prompt provided by the user. This can be assessed using automated methods like F-score, BLEU score, ROUGE, etc., which compare generated responses with ground truth data from human experts. However, these scores may not always correlate perfectly with perceived QoE due to their limitations.\\n2. **Latency/Speed**: Latency refers to the time taken between submitting a request and receiving a complete answer. Users generally expect quick results when interacting with AI systems. Low latencies contribute positively towards better UX while high ones result in frustration and dissatisfaction among users. Measuring this aspect requires monitoring system performance indicators such as average processing times across different scenarios.\\n3. **Coherence & Consistency**: Coherent text generation involves producing continuous passages where ideas flow naturally without abrupt changes in topic or\", 'status': True, 'server_code': 1}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the variable\n",
    "\n",
    "\n",
    "# Construct the payload with the variable\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"{}\".format(query)\n",
    "        }\n",
    "    ],\n",
    "    \"system_prompt\": \"\",\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 5,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_tokens\": 256,\n",
    "    \"web_access\": False\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    \"x-rapidapi-key\": \"5c75ae5d92msh98cc178e78ace9dp18774cjsn463b1487b11e\",\n",
    "    \"x-rapidapi-host\": \"chatgpt-42.p.rapidapi.com\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
